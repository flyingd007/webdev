{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U18j7UUmGg1A",
        "outputId": "c73033f9-857f-4f83-fd3b-d25520b98efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install flask flask-ngrok pyngrok sentence-transformers faiss-cpu spacy fuzzywuzzy python-Levenshtein -q\n",
        "!python -m spacy download en_core_web_md -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "W7m8xojiH-8k",
        "outputId": "643439eb-f41e-49d7-8a20-cba368b88992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-25 11:38:07,987 - INFO - Logging initialized.\n",
            "2025-04-25 11:38:08,115 - INFO - Ngrok authtoken set.\n",
            "2025-04-25 11:38:10,159 - INFO - spaCy model loaded.\n",
            "2025-04-25 11:38:10,165 - INFO - Use pytorch device_name: cuda\n",
            "2025-04-25 11:38:10,166 - INFO - Load pretrained SentenceTransformer: multi-qa-MiniLM-L6-cos-v1\n",
            "2025-04-25 11:38:11,264 - INFO - Model, FAISS index, and documents loaded.\n",
            "2025-04-25 11:38:11,268 - INFO - Flask app initialized.\n",
            "2025-04-25 11:38:11,271 - INFO - Working directory: /content\n",
            "2025-04-25 11:38:11,272 - INFO - Opening tunnel named: http-5000-b5f1dea8-b1e8-4d6e-867d-4482643eb2d5\n",
            "2025-04-25 11:38:11,294 - INFO - t=2025-04-25T11:38:11+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2025-04-25 11:38:11,301 - INFO - t=2025-04-25T11:38:11+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "2025-04-25 11:38:11,303 - INFO - t=2025-04-25T11:38:11+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "2025-04-25 11:38:11,316 - INFO - t=2025-04-25T11:38:11+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "2025-04-25 11:38:11,536 - ERROR - t=2025-04-25T11:38:11+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "2025-04-25 11:38:11,538 - ERROR - t=2025-04-25T11:38:11+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "2025-04-25 11:38:11,539 - INFO - t=2025-04-25T11:38:11+0000 lvl=info msg=\"received stop request\" obj=app stopReq=\"{err:{Remote:true Inner:{Inner:0xc0006b0b00}} restart:false}\"\n",
            "2025-04-25 11:38:11,540 - ERROR - t=2025-04-25T11:38:11+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "2025-04-25 11:38:11,541 - CRITICAL - t=2025-04-25T11:38:11+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c95222a1b525>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Working directory: {os.getcwd()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0mstart_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0mflask_thread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_flask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0mflask_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c95222a1b525>\u001b[0m in \u001b[0;36mstart_ngrok\u001b[0;34m()\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstart_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m     \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbind_tls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'ngrok tunnel: {public_url}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    429\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle\n",
        "from flask import Flask, request, jsonify\n",
        "import logging\n",
        "from fuzzywuzzy import fuzz\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "# Set working directory\n",
        "os.chdir('/content/')\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),\n",
        "        logging.FileHandler('/content/server.log')\n",
        "    ],\n",
        "    force=True\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.getLogger().handlers[0].flush = sys.stdout.flush\n",
        "logger.info(\"Logging initialized.\")\n",
        "\n",
        "# Set ngrok authtoken\n",
        "NGROK_AUTHTOKEN = \"2wDS1aUaRLWL8dDnEQSzqx1lZzr_2fpDeY489rtEugiAfwX1A\"\n",
        "os.system(f\"ngrok authtoken {NGROK_AUTHTOKEN}\")\n",
        "logger.info(\"Ngrok authtoken set.\")\n",
        "\n",
        "# Load spaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_lg\")\n",
        "    logger.info(\"spaCy model loaded.\")\n",
        "except OSError:\n",
        "    os.system(\"python -m spacy download en_core_web_lg\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    logger.info(\"spaCy model downloaded and loaded.\")\n",
        "\n",
        "# Define column mapping\n",
        "column_mapping = {\n",
        "    \"revenue\": \"txAmount\", \"transaction date\": \"txDate\", \"user id\": \"customerId\", \"status\": \"txStatus\",\n",
        "    \"transaction status\": \"txStatus\", \"email\": \"emailId\", \"order number\": \"orderNo\", \"payment status\": \"txStatus\",\n",
        "    \"amount\": \"txAmount\", \"payment mode\": \"paymentMode\", \"created on\": \"CreatedOn\", \"updated on\": \"UpdatedOn\",\n",
        "    \"bank name\": \"bankName\", \"currency\": \"currency\", \"card type\": \"cardType\", \"pg transaction id\": \"pgTxId\",\n",
        "    \"bank transaction id\": \"bankTxId\", \"proposal number\": \"proposalNo\", \"item id\": \"itemId\", \"lead id\": \"leadId\",\n",
        "    \"settled amount\": \"SettledAmount\", \"parent transaction ref id\": \"ParentTxRefId\", \"transaction type\": \"TxTypeId\",\n",
        "    \"plan name\": \"PlanName\", \"settlement flag\": \"settlementFlag\", \"hold payment\": \"holdPayment\", \"premium\": \"premium\",\n",
        "    \"attempt id\": \"id\", \"order id\": \"orderNo\", \"pg order id\": \"pg_orderId\", \"payment gateway\": \"pbGatewayName\",\n",
        "    \"merchant id\": \"merchantId\", \"transaction amount\": \"txAmount\", \"checksum\": \"pgChecksum\",\n",
        "    \"response message\": \"responseMessage\", \"user credentials\": \"user_credentials\", \"subvention amount\": \"SubventionAmount\",\n",
        "    \"emi\": \"IsEMI\", \"payment frequency\": \"payFrequency\", \"agent id\": \"agentId\", \"account number\": \"accountNo\",\n",
        "    \"client ip\": \"clientIP\", \"capture release\": \"capture_release_at\", \"call back url\": \"buCallbackSuccessUrl\",\n",
        "    \"is authorized\": \"isAuthorised\", \"bank id\": \"bankId\", \"gateway name\": \"pgGatewayName\",\n",
        "    \"pg request type\": \"pgRequestType\", \"website\": \"pgWebsite\", \"callback url\": \"pgiCallbackUrl\", \"salt\": \"salt\",\n",
        "    \"user agent\": \"UserAgent\", \"is no cost emi\": \"IsNoCostEMI\", \"payment reference id\": \"paymentRefId\",\n",
        "    \"payment term\": \"payTerm\", \"callback success url\": \"buCallbackSuccessUrl\", \"callback failure url\": \"buCallbackFailureUrl\",\n",
        "    \"session time\": \"session_time\", \"capture release by\": \"capture_release_by\", \"customer email\": \"emailId\",\n",
        "    \"customer mobile\": \"mobileNo\", \"pb payment mode\": \"pbPaymentMode\", \"merchant key\": \"merchantKey\",\n",
        "    \"is seamless\": \"isSeamless\", \"channel\": \"channel\", \"pre auth\": \"preAuth\", \"token name\": \"tokenName\",\n",
        "    \"account id\": \"accountId\", \"capture release flag\": \"callBackFlag\", \"final status updated at\": \"finalStatusUpdatedAt\",\n",
        "    \"is affiliate\": \"isAffiliate\", \"max recurring amount\": \"maxRecurringAmount\", \"is skip auto register\": \"isSkipAutoRegister\"\n",
        "}\n",
        "\n",
        "# Define txStatus mapping\n",
        "tx_status_mapping = {\n",
        "    -4: \"MANDATE_UPDATE_SUCCESS\", -3: \"TXN_AUTHORIZED\", -2: \"TXN_REQUESTED\", -1: \"AMBIGUITY_STATE\",\n",
        "    0: \"Default\", 1: \"TXN_INITIATED\", 2: \"TXN_FAILURE\", 3: \"TXN_SUCCESS\", 4: \"TXN_PENDING\", 5: \"MONEY_WITH_PB\",\n",
        "    6: \"RELEASE_PAYMENT_INITIATED\", 7: \"RELEASE_PAYMENT_APPROVED\", 8: \"RELEASE_PAYMENT_REJECTED\",\n",
        "    9: \"SETTLEMENT_INITIATED\", 10: \"SETTLEMENT_FAILURE\", 11: \"SETTLEMENT_SUCCESS\", 12: \"SETTLEMENT_COMPLETED\",\n",
        "    13: \"REFUND_INITIATED\", 14: \"REFUND_APPROVED\", 15: \"PARTIAL_REFUND_FROM_SUCCESS\",\n",
        "    16: \"REFUND_INITIATED_TO_NODAL\", 17: \"REFUND_SUCCESS_BY_NODAL\", 18: \"REFUND_FAILURE_BY_NODAL\",\n",
        "    19: \"REFUND_INITIATED_TO_PG\", 20: \"REFUND_SUCCESS_BY_PG\", 21: \"REFUND_FAILURE_BY_PG\",\n",
        "    22: \"SPLIT_SETTLEMENT_INITIATED\", 23: \"SPLIT_SETTLEMENT_RELEASED\", 24: \"PARTIAL_REFUND_FROM_STLMNT\",\n",
        "    25: \"REFUND_AFTER_SETTLEMENT\", 26: \"REFUND_REQUESTED\", 27: \"RECURRING_INITIATED\", 28: \"TXN_RELEASED\",\n",
        "    29: \"TXN_CAPTURED\", 30: \"RECURRING_CREATED\", 31: \"NO_RESPONSE_FROM_BANK\", 32: \"SMART_COLLECTION_INITIATED\",\n",
        "    33: \"PARTIAL_SMART_COLLECTION\", 34: \"SMART_COLLECTION_COMPLETED\", 35: \"OTM_SUCCESS\", 36: \"OTM_CAPTURED\",\n",
        "    98: \"CHARGEBACK\", 99: \"CHARGEBACK\", 100: \"INVALID_INPUT\", 101: \"ORDER_ITEM_PROCESSED\", 102: \"COD_INITIATED\",\n",
        "    103: \"COD_PENDING\", 104: \"COD_APPOINTMENT_CONFIRMED\", 105: \"COD_IN_TRANSIT\", 106: \"COD_DELIVERED\",\n",
        "    107: \"COD_APPOINTMENT_RESCHEDULED\", 108: \"COD_BLIND_ATTEMPT\", 109: \"COD_CANCELLED\", 1001: \"SI_INITIATED\",\n",
        "    1002: \"SI_SUCCESS\", 1003: \"SI_FAILURE\", 1004: \"SI_CANCELLED\", 1005: \"PRENOTIFY_FAILED\",\n",
        "}\n",
        "\n",
        "# Define product_mapping\n",
        "product_mapping = {\n",
        "    2: \"Health\", 130: \"Super Topup\", 131: \"SME/GMC\", 132: \"Auto Loan\", 135: \"Debit Card\", 136: \"Credit Bureau\",\n",
        "    137: \"TwoWheelerLoan\", 138: \"Cancer Insurance\", 139: \"Commercial\", 140: \"Dengue Care\", 141: \"Wallet\",\n",
        "    142: \"Secured Card\", 143: \"Heart\", 144: \"Heart and Cancer\", 145: \"Renewal Life\",\n",
        "    146: \"Renewal Investment\", 147: \"Renewal Health\", 148: \"International Car\", 149: \"HealthCaptive\",\n",
        "    150: \"International Travel\", 151: \"International Health\", 152: \"International Home\",\n",
        "    153: \"International SME Package\", 154: \"CyberSafe\", 155: \"International Term\",\n",
        "    156: \"International Investment\", 157: \"International Pension\", 158: \"International Credit Cards\",\n",
        "    159: \"International Personal Loans\", 160: \"International Bank Accounts\", 161: \"Mobile Insurance\",\n",
        "    163: \"International Bike Insurance\", 176: \"International Commercial Vehicles\",\n",
        "    177: \"Loss Of Income\", 178: \"International Tele Consultation\", 186: \"Car Insurance\",\n",
        "    187: \"Two wheeler\", 188: \"Commercial Vehicle\", 189: \"PB Partner PA\", 190: \"PBPartner Health\",\n",
        "    191: \"International Warranty\", 193: \"POSP SME\", 194: \"PBP Home\", 195: \"Pet Insurance\",\n",
        "    200: \"PBP Investments\", 201: \"International Bike Insurance\", 216: \"Health on Life\",\n",
        "    255: \"Others\", 501: \"Paisa Bazaar\"\n",
        "}\n",
        "\n",
        "time_phrases = {\n",
        "    \"last month\": \"CreatedOn BETWEEN DATE_SUB(CURRENT_DATE, INTERVAL 60 DAY) AND DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY)\",\n",
        "    \"this month\": \"CreatedOn >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY)\",\n",
        "    \"yesterday\": \"CreatedOn = DATE_SUB(CURRENT_DATE, INTERVAL 1 DAY)\",\n",
        "    \"last week\": \"CreatedOn BETWEEN DATE_SUB(CURRENT_DATE, INTERVAL 14 DAY) AND DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY)\",\n",
        "    \"this week\": \"CreatedOn >= DATE_SUB(CURRENT_DATE, INTERVAL 7 DAY)\",\n",
        "    \"last quarter\": \"CreatedOn BETWEEN DATE_SUB(CURRENT_DATE, INTERVAL 6 MONTH) AND DATE_SUB(CURRENT_DATE, INTERVAL 3 MONTH)\",\n",
        "    \"this year\": \"CreatedOn >= DATE_SUB(CURRENT_DATE, INTERVAL 1 YEAR)\"\n",
        "}\n",
        "\n",
        "# Document structure\n",
        "document_objs = [\n",
        "    {\n",
        "        \"schema\": (\n",
        "            f\"Table: payment_details | Columns: id, pbGatewayName, orderId, customerId, txAmount, mobile, email, pgTxId, bankTxId, currency, responseCode, txDate, bankName, paymentMode, txStatus, cardType, CreatedOn, UpdatedOn, productId, orderNo, settlementFlag, premium | \"\n",
        "            f\"product Mapping: {product_mapping}\"\n",
        "        ),\n",
        "        \"sample_queries\": [\n",
        "            \"SELECT COUNT(*) FROM payment_details WHERE txStatus = 3 #TXN_SUCCESS\",\n",
        "            \"SELECT txStatus, COUNT(*) FROM payment_details GROUP BY txStatus ORDER BY COUNT(*) DESC\",\n",
        "            \"SELECT bankName, SUM(txAmount) FROM payment_details WHERE txDate >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY) GROUP BY bankName\",\n",
        "            \"SELECT * FROM payment_details WHERE txAmount IS NULL OR txAmount = 0\",\n",
        "            \"SELECT DISTINCT paymentMode FROM payment_details\",\n",
        "            \"SELECT txDate, SUM(txAmount) AS daily_amount FROM payment_details GROUP BY txDate ORDER BY txDate DESC\",\n",
        "            \"SELECT * FROM payment_details WHERE productId IN (501, 2, 130) AND txStatus IN (3, 4) AND txDate BETWEEN '2024-12-01' AND '2024-12-31'\",\n",
        "            \"SELECT * FROM payment_details WHERE email LIKE '%gmail.com' AND txStatus = 3\",\n",
        "            \"SELECT * FROM payment_details WHERE responseCode IS NOT NULL AND LENGTH(responseCode) > 3\",\n",
        "            \"SELECT MAX(txAmount) AS max_amount, MIN(txAmount) AS min_amount FROM payment_details WHERE txStatus = 3\",\n",
        "            \"SELECT * FROM payment_details WHERE productId = 501 AND txDate BETWEEN DATE_SUB(CURRENT_DATE, INTERVAL 60 DAY) AND DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY) AND txStatus=3\",\n",
        "            \"SELECT * FROM payment_details WHERE productId = 501 AND txDate >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY)\",\n",
        "            \"SELECT * FROM payment_details WHERE orderNo = '12345'\",\n",
        "            \"SELECT SUM(txAmount) FROM payment_details\",\n",
        "            \"SELECT * FROM payment_details WHERE premium > 10000 AND settlementFlag = 1\",\n",
        "            \"SELECT * FROM payment_details WHERE CreatedOn > UpdatedOn\",\n",
        "            \"SELECT customerId, SUM(txAmount) FROM payment_details WHERE txStatus = 3 GROUP BY customerId HAVING SUM(txAmount) > 100000\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"schema\": (\n",
        "            f\"Table: payment_attempts | Columns: id, orderNo, pbGatewayName, merchantId, txAmount, pgTxId, bankTxId, currency, responseCode, txDate, bankName, paymentMode, txStatus, cardType, CreatedOn | \"\n",
        "            f\"product Mapping: {product_mapping}\"\n",
        "        ),\n",
        "        \"sample_queries\": [\n",
        "            \"SELECT * FROM payment_attempts WHERE txAmount > 10000 AND txStatus = 3\",\n",
        "            \"SELECT COUNT(*) FROM payment_attempts WHERE txDate BETWEEN '2024-01-01' AND '2024-03-31'\",\n",
        "            \"SELECT paymentMode, COUNT(*) FROM payment_attempts GROUP BY paymentMode\",\n",
        "            \"SELECT * FROM payment_attempts WHERE bankName IS NULL OR bankName = ''\",\n",
        "            \"SELECT SUM(txAmount) FROM payment_attempts WHERE txStatus = 3\",\n",
        "            \"SELECT bankName, AVG(txAmount) FROM payment_attempts WHERE txStatus = 3 GROUP BY bankName\",\n",
        "            \"SELECT * FROM payment_attempts WHERE orderNo = 'ORD12345'\",\n",
        "            \"SELECT * FROM payment_attempts WHERE CreatedOn BETWEEN '2024-12-01' AND '2024-12-31' ORDER BY CreatedOn DESC\",\n",
        "            \"SELECT * FROM payment_attempts WHERE txStatus IN (3, 4, 5)\",\n",
        "            \"SELECT COUNT(DISTINCT customerId) FROM payment_attempts\",\n",
        "            \"SELECT * FROM payment_attempts WHERE merchantId IS NOT NULL AND LENGTH(merchantId) > 5\",\n",
        "            \"SELECT merchantId, SUM(txAmount) AS total_value FROM payment_attempts GROUP BY merchantId HAVING total_value > 50000\",\n",
        "            \"SELECT txDate, COUNT(*) FROM payment_attempts WHERE txStatus = 3 GROUP BY txDate\",\n",
        "            \"SELECT txStatus, COUNT(*) FROM payment_attempts GROUP BY txStatus ORDER BY COUNT(*) DESC\",\n",
        "            \"SELECT * FROM payment_attempts WHERE txDate >= DATE_SUB(CURRENT_DATE, INTERVAL 30 DAY)\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Preprocess query\n",
        "def preprocess_query(query):\n",
        "    logger.info(f\"Preprocessing query: {query}\")\n",
        "    query = query.lower().strip()\n",
        "    doc = nlp(query)\n",
        "\n",
        "    mapped_column = None\n",
        "    aggregation = None\n",
        "    mapped_tx_status = None\n",
        "\n",
        "    for token in doc:\n",
        "        lemma = token.lemma_\n",
        "        if lemma in column_mapping:\n",
        "            mapped_column = column_mapping[lemma]\n",
        "            logger.info(f\"Mapped column: {lemma} -> {mapped_column}\")\n",
        "        if lemma in [\"total\", \"sum\", \"revenue\"]:\n",
        "            aggregation = \"SUM\"\n",
        "            mapped_column = \"txAmount\"\n",
        "            logger.info(f\"Detected aggregation: {aggregation} on {mapped_column}\")\n",
        "\n",
        "    # Detect transaction status\n",
        "    for status_id, status_name in tx_status_mapping.items():\n",
        "        if status_name.lower() in query or str(status_id) in query:\n",
        "            mapped_tx_status = status_id\n",
        "            mapped_column = \"txStatus\" if not mapped_column else mapped_column\n",
        "            query = query.replace(status_name.lower(), str(status_id))\n",
        "            logger.info(f\"Mapped transaction status: {status_name} -> {status_id}\")\n",
        "            break\n",
        "        if not mapped_tx_status:  # Fuzzy matching for status\n",
        "          for status_id, status_name in tx_status_mapping.items():\n",
        "            if fuzz.partial_ratio(status_name.lower(), query) > 80 or fuzz.partial_ratio(\"success\", query) > 80:\n",
        "                mapped_tx_status = status_id\n",
        "                mapped_column = \"txStatus\" if not mapped_column else mapped_column\n",
        "                query = query.replace(status_name.lower(), str(status_id))\n",
        "                logger.info(f\"Fuzzy matched transaction status: {status_name} -> {status_id}\")\n",
        "                break\n",
        "\n",
        "    mapped_product = None\n",
        "    for product_id, product_name in product_mapping.items():\n",
        "        if product_name.lower() in query or str(product_id) in query:\n",
        "            mapped_product = product_id\n",
        "            mapped_column = \"productId\" if not mapped_column else mapped_column\n",
        "            query = query.replace(product_name.lower(), str(product_id))\n",
        "            logger.info(f\"Mapped product: {product_name} -> {product_id}\")\n",
        "            break\n",
        "\n",
        "    if not mapped_column:\n",
        "        for key in column_mapping:\n",
        "            if fuzz.partial_ratio(key, query) > 80:\n",
        "                mapped_column = column_mapping[key]\n",
        "                logger.info(f\"Fuzzy matched column: {key} -> {mapped_column}\")\n",
        "                break\n",
        "\n",
        "    time_filter = None\n",
        "    for phrase, sql_filter in time_phrases.items():\n",
        "        if phrase in query:\n",
        "            time_filter = sql_filter.replace(\"txDate\", \"CreatedOn\")\n",
        "            logger.info(f\"Detected time phrase: {phrase} -> {time_filter}\")\n",
        "            break\n",
        "    if not time_filter and \"yesterday\" in query:\n",
        "        time_filter = \"CreatedOn = DATE_SUB(CURRENT_DATE, INTERVAL 1 DAY)\"\n",
        "        logger.info(f\"Custom time filter: {time_filter}\")\n",
        "\n",
        "    return query, mapped_column, mapped_product, time_filter, aggregation, mapped_tx_status\n",
        "\n",
        "# Generate FAISS index\n",
        "def generate_faiss_index(documents):\n",
        "    logger.info(\"Generating FAISS index...\")\n",
        "    model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
        "\n",
        "    schema_texts = [doc[\"schema\"] for doc in documents]\n",
        "    query_texts = [\" || \".join(doc[\"sample_queries\"]) for doc in documents]\n",
        "\n",
        "    schema_vectors = model.encode(schema_texts, normalize_embeddings=True)\n",
        "    query_vectors = model.encode(query_texts, normalize_embeddings=True)\n",
        "\n",
        "    combined_vectors = 0.2 * schema_vectors + 0.8 * query_vectors\n",
        "\n",
        "    dim = len(combined_vectors[0])\n",
        "    index = faiss.IndexHNSWFlat(dim, 32)\n",
        "    index.hnsw.efConstruction = 200\n",
        "    index.hnsw.efSearch = 40\n",
        "    index.add(np.array(combined_vectors))\n",
        "\n",
        "    faiss.write_index(index, \"/content/faiss_index.index\")\n",
        "    with open(\"/content/doc_mapping.pkl\", \"wb\") as f:\n",
        "        pickle.dump(documents, f)\n",
        "    logger.info(\"FAISS index and document mapping saved.\")\n",
        "\n",
        "# Generate index if needed\n",
        "if not os.path.exists(\"/content/faiss_index.index\") or not os.path.exists(\"/content/doc_mapping.pkl\"):\n",
        "    generate_faiss_index(document_objs)\n",
        "\n",
        "# Load model and index\n",
        "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
        "index = faiss.read_index('/content/faiss_index.index')\n",
        "with open('/content/doc_mapping.pkl', 'rb') as f:\n",
        "    documents = pickle.load(f)\n",
        "logger.info(\"Model, FAISS index, and documents loaded.\")\n",
        "\n",
        "# Initialize Flask\n",
        "app = Flask(__name__)\n",
        "logger.info(\"Flask app initialized.\")\n",
        "\n",
        "# Health check\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    logger.info(\"Health check called.\")\n",
        "    return jsonify({'status': 'Server is running', 'working_dir': os.getcwd()}), 200\n",
        "\n",
        "# Embed-query endpoint\n",
        "@app.route('/embed-query', methods=['POST'])\n",
        "def embed_query():\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        logger.info(f\"Incoming data: {data}\")\n",
        "\n",
        "        if not data or 'query' not in data:\n",
        "            logger.error(\"Query is required.\")\n",
        "            return jsonify({'error': 'Query is required'}), 400\n",
        "\n",
        "        query = data['query']\n",
        "        logger.info(f\"Original query: {query}\")\n",
        "\n",
        "        # Preprocess\n",
        "        mapped_query, mapped_column, mapped_product, time_filter, aggregation, mapped_tx_status = preprocess_query(query)\n",
        "        logger.info(f\"Mapped query: {mapped_query}, Column: {mapped_column}, Product: {mapped_product}, Time: {time_filter}, Aggregation: {aggregation}, TxStatus: {mapped_tx_status}\")\n",
        "\n",
        "        # Encode query\n",
        "        query_vector = model.encode([mapped_query], normalize_embeddings=True)\n",
        "\n",
        "        # FAISS search\n",
        "        k = 5\n",
        "        scores, indices = index.search(np.array(query_vector), k)\n",
        "        logger.info(f\"FAISS indices: {indices}, Scores: {scores}\")\n",
        "\n",
        "        similarities = 1 - (scores / 2)\n",
        "        threshold = 0.25\n",
        "\n",
        "        relevant_docs = []\n",
        "        for idx, sim in zip(indices[0], similarities[0]):\n",
        "            if idx >= len(documents) or sim < threshold:\n",
        "                logger.warning(f\"Skipping index {idx}: similarity {sim} below threshold or out of bounds\")\n",
        "                continue\n",
        "\n",
        "            doc = documents[idx]\n",
        "\n",
        "            # Rank sample queries\n",
        "            def score_query(q):\n",
        "                score = 0\n",
        "                if mapped_column and mapped_column in q:\n",
        "                    score += 5\n",
        "                if mapped_product and str(mapped_product) in q:\n",
        "                    score += 4\n",
        "                if mapped_tx_status and str(mapped_tx_status) in q:\n",
        "                    score += 4\n",
        "                if time_filter and time_filter in q:\n",
        "                    score += 3\n",
        "                elif \"CreatedOn\" in q or \"txDate\" in q:\n",
        "                    score += 1\n",
        "                if aggregation and \"SUM\" in q:\n",
        "                    score += 5\n",
        "                return score\n",
        "\n",
        "            ranked_queries = sorted(doc[\"sample_queries\"], key=score_query, reverse=True)\n",
        "            logger.info(f\"Ranked queries: {[f'{q} (score: {score_query(q)})' for q in ranked_queries[:3]]}\")\n",
        "\n",
        "            # Construct SQL query\n",
        "            sample_query = ranked_queries[0]\n",
        "            if aggregation and (mapped_product or mapped_tx_status):\n",
        "                table_name = \"payment_details\" if \"payment_details\" in doc[\"schema\"] else \"payment_attempts\"\n",
        "                alias = \"Total_Revenue\" if aggregation == \"SUM\" and mapped_column == \"txAmount\" else mapped_column\n",
        "                sample_query = f\"SELECT {aggregation}({mapped_column}) AS {alias} FROM {table_name}\"\n",
        "                conditions = []\n",
        "                if mapped_product:\n",
        "                    conditions.append(f\"productId = {mapped_product}\")\n",
        "                if mapped_tx_status:\n",
        "                    conditions.append(f\"txStatus = {mapped_tx_status}\")\n",
        "                if time_filter:\n",
        "                    conditions.append(time_filter)\n",
        "                if conditions:\n",
        "                    sample_query += \" WHERE \" + \" AND \".join(conditions)\n",
        "                sample_query += \";\"\n",
        "                logger.info(f\"Constructed query: {sample_query}\")\n",
        "            else:\n",
        "                if mapped_product and str(mapped_product) not in sample_query:\n",
        "                    sample_query += f\" AND productId = {mapped_product}\"\n",
        "                    logger.info(f\"Added product filter: {sample_query}\")\n",
        "                if mapped_tx_status and str(mapped_tx_status) not in sample_query:\n",
        "                    sample_query += f\" AND txStatus = {mapped_tx_status}\"\n",
        "                    logger.info(f\"Added txStatus filter: {sample_query}\")\n",
        "                if time_filter and (\"txDate\" in sample_query or \"CreatedOn\" in sample_query) and time_filter not in sample_query:\n",
        "                    sample_query = re.sub(r'(txDate|CreatedOn)\\s*(>=|BETWEEN)\\s*[^)]+\\)', time_filter, sample_query)\n",
        "                    logger.info(f\"Adjusted time filter: {sample_query}\")\n",
        "\n",
        "            # Boost similarity\n",
        "            similarity_boost = sim\n",
        "            if \"txAmount\" in doc[\"schema\"] and \"productId\" in doc[\"schema\"]:\n",
        "                similarity_boost += 0.25\n",
        "                logger.info(f\"Boosted similarity: {sim} -> {similarity_boost}\")\n",
        "\n",
        "            # Prepare mappings (optimized)\n",
        "            mappings = {}\n",
        "            if mapped_product:\n",
        "                mappings[\"product_mapping\"] = {mapped_product: product_mapping[mapped_product]}  # Include only the relevant product\n",
        "            if mapped_tx_status:\n",
        "                mappings[\"tx_status_mapping\"] = {mapped_tx_status: tx_status_mapping[mapped_tx_status]}  # Include only the relevant status\n",
        "\n",
        "            relevant_docs.append({\n",
        "                \"schema\": doc[\"schema\"],\n",
        "                \"sample_query\": sample_query,\n",
        "                \"mappings\": mappings,  # Only relevant mappings\n",
        "                \"similarity\": float(similarity_boost)\n",
        "            })\n",
        "\n",
        "        if not relevant_docs:\n",
        "            logger.warning(\"No relevant documents found\")\n",
        "            return jsonify({\n",
        "                'error': 'No relevant schemas found',\n",
        "                'original_query': query,\n",
        "            }), 404\n",
        "\n",
        "        relevant_docs = sorted(relevant_docs, key=lambda x: x[\"similarity\"], reverse=True)[:2]\n",
        "        logger.info(f\"Response: {relevant_docs}\")\n",
        "\n",
        "        return jsonify({\n",
        "            'relevant_docs': relevant_docs,\n",
        "            'original_query': query,\n",
        "            'mapped_query': mapped_query,\n",
        "            'mapped_column': mapped_column,\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Exception: {str(e)}\")\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "# Start ngrok\n",
        "def start_ngrok():\n",
        "    ngrok.kill()\n",
        "    public_url = ngrok.connect(5000, bind_tls=True)\n",
        "    logger.info(f'ngrok tunnel: {public_url}')\n",
        "    return public_url\n",
        "\n",
        "# Run Flask\n",
        "def run_flask():\n",
        "    logger.info(\"Starting Flask...\")\n",
        "    app.run(host='0.0.0.0', port=5000, use_reloader=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    logger.info(f\"Working directory: {os.getcwd()}\")\n",
        "    start_ngrok()\n",
        "    flask_thread = threading.Thread(target=run_flask)\n",
        "    flask_thread.start()\n",
        "    while True:\n",
        "        time.sleep(1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}